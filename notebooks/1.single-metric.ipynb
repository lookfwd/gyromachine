{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Welcome. Introduction to Spark and the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import re\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from utils import start_spark_session, get_s3, list_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset lives in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = list_s3()\n",
    "print(len(lf), 'files')\n",
    "lf[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_s3('configs/0/batch_00.log.gz', 'tmp.log.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('tmp.log.gz', 'rt') as f:\n",
    "    for line in f.readlines()[:5]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "\n",
    "```\n",
    "time, cycle, conf, run, x, y, z\n",
    "```\n",
    "\n",
    "We will see what they mean in a bit. For now let's write some code that automates parsing those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(src):\n",
    "    dst = \"tmp.log.gz\"\n",
    "    get_s3(src, dst)\n",
    "    with gzip.open(dst, 'rt') as f:\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            time, cycle, conf, run, x, y, z = re.split('[:,]', line.strip())\n",
    "            cycle, conf, run = map(int, (cycle, conf, run))\n",
    "            time, x, y, z = map(float, (time, x, y, z))\n",
    "\n",
    "            count += 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code above is about 12 lines, and there are several things missing:\n",
    "\n",
    "    * No cleanup\n",
    "    * No retries in case of failure\n",
    "    * Not able to process many files in parallel\n",
    "    * Explicitly tailored to gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "count = process('configs/0/batch_00.log.gz')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah tmp.log.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some seconds spent. We would project about a minute for 10 such files, if the other ones have the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Spark. We start the Spark session with a utility function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can link on the 'Spark UI' link below. Spent some time to explore the screen. It will become more interesting soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do exactly the same thing as before with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "base_df = spark.read.text('s3a://enginestream/configs/0/batch_00.log.gz')\n",
    "print(base_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it for all (10) files in that directory (by using the '*' wildcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "base_df = spark.read.text('s3a://enginestream/configs/0/batch_*.log.gz')\n",
    "print(base_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't take 70 seconds. Why? Because\n",
    "\n",
    "1. it uses concurrency and\n",
    "2. it runs Scala\n",
    "\n",
    "We can do full parsing with code similar but slightly more expressive to the pure Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = (base_df\n",
    "           .select(F.split('value', '[:,]').alias('cols'))\n",
    "           .select(F.expr(\"cols[0]\").cast(\"float\").alias('time'),\n",
    "                   F.expr(\"cols[1]\").cast(\"long\").alias('cycle'),\n",
    "                   F.expr(\"cols[2]\").cast(\"long\").alias('conf'),\n",
    "                   F.expr(\"cols[3]\").cast(\"long\").alias('run'),\n",
    "                   F.expr(\"cols[4]\").cast(\"float\").alias('x'),\n",
    "                   F.expr(\"cols[5]\").cast(\"float\").alias('y'),\n",
    "                   F.expr(\"cols[6]\").cast(\"float\").alias('z'))\n",
    "           .drop(\"cols\"))\n",
    "\n",
    "logs_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily get Pandas dataframes that look better and allow easy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `logs_df` data has a schema implicitly defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write SQL queries using the API (i.e. Python). It will run against the entire dataset right now (all 10 files).\n",
    "\n",
    "```\n",
    "Compressed size: 273 Mb\n",
    "Uncompressed size: 832 Mb\n",
    "Rows: 17,222,161\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.select(F.min('run'),\n",
    "               F.max('run'),\n",
    "               F.min('cycle'),\n",
    "               F.max('cycle')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can use the usual SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.createOrReplaceTempView(\"logs\")\n",
    "\n",
    "pdf = (spark.sql(\"SELECT MIN(run), MAX(run), MIN(cycle), MAX(cycle)\"\n",
    "                 \"FROM logs\")\n",
    "       .toPandas())\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the 'run' argument. How many rows are there for every run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = (spark.sql(\"SELECT run, count(1) AS cnt \"\n",
    "                 \"FROM logs \"\n",
    "                 \"GROUP BY run \"\n",
    "                 \"ORDER BY cnt DESC\")\n",
    "       .toPandas())\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all data for the smallest run to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.sql(\"SELECT * FROM logs WHERE run = 2001\").toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's a Pandas dataframe, it's trivial to plot it or do a histogram of e.g. the `x` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pdf['x'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pdf['x'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot more the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "ax[0][0].plot(pdf['cycle'])\n",
    "ax[0][1].plot(pdf['x'])\n",
    "ax[1][0].plot(pdf['y'])\n",
    "ax[1][1].plot(pdf['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in in `x` and `y`, for the first few cycles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = 1800\n",
    "\n",
    "for i in ('x', 'y'):\n",
    "    plt.plot(pdf[i][:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be out-of-phase with a consistent way. Let's plot them in two-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = 2500\n",
    "offset = 2\n",
    "\n",
    "plt.plot(pdf['x'][:end], pdf['y'][:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. They are connected, and have a consistent relationship, since they're ellipses. But their centers seem to move in and out. Maybe we can see their change in time by using the cycle variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 35000\n",
    "end = 40000\n",
    "offset = 2\n",
    "\n",
    "plt.plot([x + offset * c\n",
    "          for x, c in zip(pdf['x'][start:end], pdf['cycle'][start:end])],\n",
    "         pdf['y'][start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - we are convinced that their ampitude is related and changes in time in a sinusoidal way. Is their frequency in time stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = 20000\n",
    "\n",
    "fs = 100\n",
    "f, t, Zxx = signal.stft(pdf['x'][:end], fs, nperseg=1000)\n",
    "plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=3)\n",
    "plt.ylim((0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, frequency goes up and down as well! What's the histogram of the duration of a cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.sql(\"SELECT MAX(time) - MIN(time) AS duration \"\n",
    "                \"FROM logs \"\n",
    "                \"WHERE run = 2001 \"\n",
    "                \"GROUP BY cycle \"\n",
    "                \"ORDER BY duration DESC\"\n",
    "               ).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['duration'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low ones can be noize. It seems that the duration is between 0.08 and 0.12 units. Wondering if that's the case for every run. Let's run it on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.sql(\"SELECT run, MAX(time) - MIN(time) AS duration \"\n",
    "                \"FROM logs \"\n",
    "                \"GROUP BY run, cycle \"\n",
    "                \"ORDER BY duration DESC\"\n",
    "               ).toPandas()\n",
    "\n",
    "pdf['duration'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have an outlier (measurement error) around 6000. Let's remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[pdf['duration'] < 6000]['duration'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[pdf['duration'] < 6000]['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pdf[pdf['duration'] < 6000]['duration'].quantile(q=0.95),\n",
    " pdf[pdf['duration'] < 6000]['duration'].quantile(q=0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so indeed the range between 0.08 and 0.12 units is the most popular. The 95th quantile is 0.17 and the 99th is 0.23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
